# -*- coding: utf-8 -*-
"""
Created on Fri Feb 13 13:51:04 2015

@author: fh235918
"""

#! /usr/bin/env python
##########################################################################
# Brainomics - Copyright (C) CEA, 2013
# Distributed under the terms of the CeCILL-B license, as published by
# the CEA-CNRS-INRIA. Refer to the LICENSE file or to
# http://www.cecill.info/licences/Licence_CeCILL-B_V1-en.html
# for details.
##########################################################################

# System import
#import numpy
#import pickle
#import os
#import optparse
#
#
#
#
#
#
##load prepared data for the project (see exemple_pw.py)
#fname = '/neurospin/brainomics/2015_hippo_l1_gl_ovl/data/synaptic10.pickle'
#f = open(fname)
#genodata = pickle.load(f)
#f.close()
#
## read x data
#x = genodata.data
#x_subj = ["%012d" % int(i) for i in genodata.fid]
#
## read y data
#y = open('/neurospin/brainomics/2015_hippo_l1_gl_ovl/data/Hippocampus_L.csv').read().split('\n')[1:-1]
#y_subj = [i.split('\t')[0] for i in y]
#y = [float(i.split('\t')[2]) for i in y]
#
##intersect subject list
#soi = list(set(x_subj).intersection(set(y_subj)))
#
## build daatset with X and Y
#X = numpy.zeros((len(soi), x.shape[1]))
#Y = numpy.zeros(len(soi))
#for i, s in enumerate(soi):
#    X[i, :] = x[x_subj.index(s), :]
#    Y[i] = y[y_subj.index(s)]
#
#groups_descr = genodata.get_meta_pws()
#groups_name = groups_descr.keys()
#groups = [list(groups_descr[n]) for n in groups_name]
#










#! /usr/bin/env python
##########################################################################
# Brainomics - Copyright (C) CEA, 2013
# Distributed under the terms of the CeCILL-B license, as published by
# the CEA-CNRS-INRIA. Refer to the LICENSE file or to
# http://www.cecill.info/licences/Licence_CeCILL-B_V1-en.html
# for details.
##########################################################################

# System import
import numpy as np
import pickle
import os
import optparse
import sklearn
import sklearn.preprocessing
import sklearn.decomposition
from sklearn.metrics import r2_score
from sklearn import cross_validation
import parsimony.algorithms as algorithms
import parsimony.utils.consts as consts
import parsimony.estimators as estimators
import parsimony.functions.nesterov.gl as gl
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
#load prepared data for the project (see exemple_pw.py)
fname = '/neurospin/brainomics/2015_hippo_l1_gl_ovl/data/synaptic10.pickle'
f = open(fname)
genodata = pickle.load(f)
f.close()

# read x data
x = genodata.data
x_subj = ["%012d" % int(i) for i in genodata.fid]

# read y data
y = open('/neurospin/brainomics/2015_hippo_l1_gl_ovl/data/Hippocampus_L.csv').read().split('\n')[1:-1]
y_subj = [i.split('\t')[0] for i in y]
y = [float(i.split('\t')[2]) for i in y]

#######################
#Covariate of non-interest variables
#############################
covariate = open('/neurospin/brainomics/2015_hippo_l1_gl_ovl/data/gender_centre.cov').read().split('\n')[1:-1]
covariate_subj = [i.split('\t')[0] for i in covariate]
covariate_data= np.asarray([[ int(float(j))  for j in i.split('\t')[2:]] for i in covariate])
indx = np.where(covariate_data[:,0]!=-9)[0]
covariate_data = covariate_data[indx,:]
covariate_subj = list(np.asarray(covariate_subj)[indx])


#intersect subject list
soi = list(set(x_subj).intersection(set(y_subj)).intersection(set(covariate_subj)))

# build daatset with X and Y
X_ = np.zeros((len(soi), x.shape[1]))
Y_ = np.zeros(len(soi))
Cov_ = np.zeros((len(soi), covariate_data.shape[1]))
for i, s in enumerate(soi):
    X_[i, :] = x[x_subj.index(s), :]
    Y_[i] = y[y_subj.index(s)]
    Cov_[i, :] = covariate_data[covariate_subj.index(s), :]

groups_descr = genodata.get_meta_pws()
groups_name = groups_descr.keys()
groups = [list(groups_descr[n]) for n in groups_name]





#######################
#selecting individuals with respect to hippo volume
#############################
hyp_vol_max=3500
hyp_vol_min=2500
ind = [i for i,temp in enumerate(Y_) if temp < hyp_vol_max and temp>hyp_vol_min]


X = np.zeros((len(ind), X_.shape[1]))
Y = np.zeros(len(ind))
Xnon_res = np.zeros((len(ind), X_.shape[1]+Cov_.shape[1]))
Ynon_res = np.zeros(len(ind))
Cov = np.zeros((len(ind), Cov_.shape[1]))

for i, s in enumerate(ind):
    Xnon_res[i, :] = np.hstack((Cov_[s, :], X_[s, :]))
    Ynon_res[i] = Y_[s]
    Cov[i,:] = Cov_[s,:]
    X[i, :] =  X_[s, :]
    Y[i] = Y_[s]


############################
#scalin and selecting train and test
#############"""


n,p=X.shape
X = sklearn.preprocessing.scale(X,
                                axis=0,
                                with_mean=True,
                                with_std=False)

Cov = sklearn.preprocessing.scale(Cov,
                                axis=0,
                                with_mean=True,
                                with_std=False)

##############"
#resudualisation
##############"""



Y=Y-Y.mean()




Y =Y - LinearRegression().fit(Cov,Y).predict(Cov) 
#Y=Y-Y.mean()








n_train =  int(X.shape[0]/1.75)
Xtr = X[:n_train, :]
ytr = Y[:n_train]
Xte = X[n_train:, :]
yte = Y[n_train:]
########################################
#test










####################################################################
#test with simple linear correlation and corrected p value. We first compute the
#linear correlation coeffition than estimate the corrected p values using an fdr classic approech
################################################################

######################
#Here we compute the linear correlation of Y with each SNP and the 
#corresponding p_value. Than we plot the sorted p_values in order to check 
# we can select a subset of SNP if the plot ha a specific form 
#####################
from scipy.stats import pearsonr
p_vect=np.array([])
cor_vect=np.array([])
for i in range(3201):
    r_row, p_value = pearsonr(X[:,i], Y)
    p_vect = np.hstack((p_vect,p_value))
    cor_vect = np.hstack((cor_vect,r_row))
p2=np.sort(p_vect)
plt.plot(p2)    
plt.show()   
#############
#The obtained plot does not allow to make any selection. This can be done later
#using another priors on different genes (Vincent, do you 
#have an idea to remove the not pertinent SNP to the subject under consideration?).  
###############

 
#############
#Here we check how many SNP are significantly (w.r.p=0.05) correlated to Y : 

indices = np.where(p_vect <= 0.05)
print len(indices[0])
# there are 126 SNP
 ###########"




################
#Here we compue the corrected p_values, all of them are equal to one!! 
#we need to work on the data again, loock for priors, normalize volumes with respect to sex.. 
#############"
import p_value_correction as p_c
p_corrected = p_c.fdr(p_vect)
indices = np.where(p_corrected <= 0.05)
#p_vect[p_vect > 0.05] = 0.









##########################
#Now we use same multivariate approaches
#############################"

#############################################################################
## sklearn
from sklearn.linear_model import ElasticNet, ElasticNetCV
from sklearn import svm
from sklearn.ensemble import RandomForestRegressor

## Enet
enet = ElasticNetCV()
enet.fit(Xtr, ytr)
y_pred_enet = enet.predict(Xte)
r2_score(yte, y_pred_enet)
print cross_validation.cross_val_score(enet, X, Y, cv=5)
#array([ -8.60231114e-05,  -6.41673920e-03,  -8.69694214e-03,
#        -6.83330203e-02,  -7.77224429e-03])

## SVM
svmlin = svm.SVR(kernel='linear')
svmlin.fit(Xtr, ytr)
y_pred_svmlin = svmlin.predict(Xte)
print r2_score(yte, y_pred_svmlin)
print cross_validation.cross_val_score(svmlin, X, Y, cv=5)
#array([-0.683538  , -0.58925786, -0.74613231, -1.04904344, -1.03914531])

## Randoom Forests
rf = RandomForestRegressor()
rf.fit(Xtr, ytr)
y_pred_rf = svmlin.predict(Xte)
print r2_score(yte, y_pred_rf)
print cross_validation.cross_val_score(rf, X, Y, cv=5)
#array([-0.09635575, -0.20115192, -0.07531322, -0.21041208, -0.04896116])

#Xte = sklearn.preprocessing.scale(Xte,
#                                axis=0,
#                                with_mean=True,
#                                with_std=False)
#Xtr = sklearn.preprocessing.scale(Xtr,
#                                axis=0,
#                                with_mean=True,
#                                with_std=False)
#ytr=ytr-ytr.mean()
#yte=yte-yte.mean()
#########


#####




################
#Here we test a group lasso version, it gives similar results
##########
###################
#estimation of the maximal l1 possible penalisation, 
#this will help to estimate the order of the others penalisations
###########


s= [np.linalg.norm(np.dot(Xtr[:,i],ytr)) for i in range(Xtr.shape[1])]
l1_max =0.1* np.max(s)/Xtr.shape[0]
print "l1 max is", l1_max
#################################

l1, l2, lgl =l1_max * np.array((0.1, 0.1, 0.01))



weights = [np.sqrt(len(group)) for group in groups]
weights = 1./np.sqrt(np.asarray(weights))



Agl = gl.linear_operator_from_groups(p, groups=groups, weights=weights)
algorithm = algorithms.proximal.CONESTA(eps=consts.TOLERANCE, max_iter=15000)
enet_gl = estimators.LinearRegressionL1L2GL(l1, l2,  lgl , Agl, algorithm=algorithm)
yte_pred_enetgl = enet_gl.fit(Xtr, ytr).predict(Xte)
print " r carré vaut",  r2_score(yte, yte_pred_enetgl)


Xnon_res = sklearn.preprocessing.scale(Xnon_res,
                                axis=0,
                                with_mean=True,
                                with_std=False)
Ynon_res = Ynon_res-Ynon_res.mean()


n_train =  int(X.shape[0]/1.75)
Xtr_res = Xnon_res[:n_train, :]
ytr_res = Ynon_res[:n_train]
Xte_res = Xnon_res[n_train:, :]
yte_res = Ynon_res[n_train:]
######################




s= [np.linalg.norm(np.dot(Xtr_res[:,i],ytr_res)) for i in range(Xtr_res.shape[1])]
l1_max =0.1* np.max(s)/Xtr_res.shape[0]
print "l1 max is", l1_max





Agl = gl.linear_operator_from_groups(p, groups=groups, weights=weights)
algorithm = algorithms.proximal.CONESTA(eps=consts.TOLERANCE, max_iter=15000)
enet_gl = estimators.LinearRegressionL1L2GL(l1, l2,  lgl , Agl, algorithm=algorithm, penalty_start=8)
yte_pred_enetgl_res = enet_gl.fit(Xtr_res, ytr_res).predict(Xte_res)
print " r carré vaut",  r2_score(yte_res, yte_pred_enetgl_res)










#p1=plt.plot(yte,marker='o')
#p2=plt.plot(yte_pred_enetgl,marker='v')
#plt.title("group_lasso_test") 
##plt.legend([p1, p2], ["beta", "beta_star"])
#plt.show()
#cf = np.corrcoef(yte.reshape(1,n-n_train),yte_pred_enetgl.reshape(1,n-n_train))[0,1]
#
#print "le coef de correlation vaut ", cf 
#
